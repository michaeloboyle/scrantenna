#!/usr/bin/env python3
"""
Free LLM Entity and Relationship Extractor for Scrantenna
Provides cost-effective alternatives to OpenAI for knowledge graph generation.
"""

import json
import re
import os
from typing import List, Dict, Optional


class ProductionFreeLLMExtractor:
    """Production-ready free LLM entity/relationship extractor."""
    
    def __init__(self, ollama_model: str = "phi3:mini"):
        self.ollama_model = ollama_model
        self.ollama_available = self._check_ollama()
        self.hf_available = self._check_huggingface()
        self.hf_pipeline = None
        
        # Initialize HF pipeline if available
        if self.hf_available:
            self._init_hf_pipeline()
    
    def _check_ollama(self) -> bool:
        """Check if Ollama is available and model exists."""
        try:
            import ollama
            ollama.show(self.ollama_model)
            return True
        except Exception:
            return False
    
    def _check_huggingface(self) -> bool:
        """Check if HuggingFace transformers is available."""
        try:
            from transformers import pipeline
            return True
        except ImportError:
            return False
    
    def _init_hf_pipeline(self):
        """Initialize HuggingFace pipeline for entity extraction."""
        try:
            from transformers import pipeline
            # Try Phi-3 Mini for text generation first
            try:
                self.hf_pipeline = pipeline(
                    "text-generation",
                    model="microsoft/Phi-3-mini-4k-instruct",
                    device_map="auto",
                    torch_dtype="auto",
                    trust_remote_code=True
                )
                self.hf_method = "phi3"
            except Exception:
                # Fallback to NER pipeline
                self.hf_pipeline = pipeline(
                    "ner",
                    model="dbmdz/bert-large-cased-finetuned-conll03-english",
                    aggregation_strategy="simple"
                )
                self.hf_method = "ner"
        except Exception as e:
            print(f"Failed to initialize HF pipeline: {e}")
            self.hf_available = False
    
    def extract_for_article(self, article: Dict, index: int = 0) -> Dict:
        """Extract entities and relationships for a news article."""
        text = f"{article.get('title', '')} {article.get('description', '')}"
        
        # Extract entities with fallback chain
        entities = self._extract_entities_with_fallback(text)
        
        # Extract relationships with fallback chain
        relationships = self._extract_relationships_with_fallback(text, entities)
        
        # Generate SVG visualization
        svg_graph = self._generate_svg_graph(entities, relationships, index)
        
        return {
            "entities": entities,
            "relationships": relationships,
            "svg": svg_graph,
            "method": self._get_active_method(),
            "confidence": self._calculate_confidence(entities)
        }
    
    def _extract_entities_with_fallback(self, text: str) -> List[Dict]:
        """Extract entities using best available method."""
        
        # Try Ollama first (best for structured output)
        if self.ollama_available:
            try:
                entities = self._ollama_extract_entities(text)
                if entities:
                    return entities
            except Exception as e:
                print(f"Ollama extraction failed: {e}")
        
        # Try HuggingFace Phi-3 or NER (good accuracy)
        if self.hf_available and self.hf_pipeline:
            try:
                if hasattr(self, 'hf_method') and self.hf_method == "phi3":
                    entities = self._phi3_extract_entities(text)
                else:
                    entities = self._hf_extract_entities(text)
                if entities:
                    return entities
            except Exception as e:
                print(f"HuggingFace extraction failed: {e}")
        
        # Fallback to rule-based extraction
        return self._rule_based_entities(text)
    
    def _ollama_extract_entities(self, text: str) -> List[Dict]:
        """Extract entities using Ollama."""
        import ollama
        
        prompt = f"""Extract named entities from this news text. Return ONLY valid JSON array format with 'name' and 'type' fields.
Entity types: PERSON, ORGANIZATION, LOCATION, DATE, EVENT

Text: {text}

JSON:"""
        
        response = ollama.generate(
            model=self.ollama_model,
            prompt=prompt,
            options={
                "temperature": 0.1,
                "top_p": 0.9,
                "num_predict": 200
            }
        )
        
        # Extract JSON from response
        response_text = response['response']
        json_match = re.search(r'\[.*?\]', response_text, re.DOTALL)
        
        if json_match:
            try:
                entities = json.loads(json_match.group(0))
                # Filter and validate entities
                return self._validate_entities(entities)
            except json.JSONDecodeError:
                return []
        
        return []
    
    def _phi3_extract_entities(self, text: str) -> List[Dict]:
        """Extract entities using Phi-3 Mini for better accuracy."""
        if not self.hf_pipeline:
            return []
        
        prompt = f"""<|user|>
Extract named entities from this news text and classify them as PERSON, ORGANIZATION, LOCATION, WORK (movies/TV/books), or EVENT.

Text: {text}

Return only a JSON array with objects containing 'name' and 'type' fields. No explanations.

Examples:
- Movies/shows: {{"name": "Final Act", "type": "WORK"}}
- People: {{"name": "Hannah Fierman", "type": "PERSON"}}
- Weather: {{"name": "Flash Flood Warning", "type": "EVENT"}}
<|end|>
<|assistant|>"""

        try:
            response = self.hf_pipeline(
                prompt,
                max_new_tokens=300,
                temperature=0.1,
                do_sample=True,
                pad_token_id=self.hf_pipeline.tokenizer.eos_token_id
            )
            
            response_text = response[0]['generated_text'].split('<|assistant|>')[-1].strip()
            
            # Extract JSON from response
            json_match = re.search(r'\[.*?\]', response_text, re.DOTALL)
            if json_match:
                import json
                entities = json.loads(json_match.group(0))
                return [e for e in entities if 'name' in e and 'type' in e]
        except Exception as e:
            print(f"Phi-3 extraction error: {e}")
        
        return []
    
    def _hf_extract_entities(self, text: str) -> List[Dict]:
        """Extract entities using HuggingFace NER."""
        if not self.hf_pipeline:
            return []
        
        ner_results = self.hf_pipeline(text)
        entities = []
        
        for result in ner_results:
            if result['score'] > 0.7:  # High confidence only
                entity_type = self._map_hf_entity_type(result['entity_group'])
                entities.append({
                    'name': result['word'].replace('##', ''),
                    'type': entity_type,
                    'confidence': result['score']
                })
        
        # Remove duplicates and clean
        return self._deduplicate_entities(entities)
    
    def _map_hf_entity_type(self, hf_type: str) -> str:
        """Map HuggingFace entity types to our schema."""
        mapping = {
            'PER': 'PERSON',
            'ORG': 'ORGANIZATION',
            'LOC': 'LOCATION',
            'MISC': 'OTHER'
        }
        return mapping.get(hf_type, 'OTHER')
    
    def _rule_based_entities(self, text: str) -> List[Dict]:
        """Enhanced fallback rule-based entity extraction with clean SVO patterns."""
        entities = []
        seen = set()
        
        # Extract quoted works/titles first (movies, shows, etc)
        quoted_works = re.findall(r'[\'"]([^\'\"]+)[\'"]', text)
        for work in quoted_works:
            if 2 < len(work) < 50 and work.lower() not in seen and not self._is_malformed_entity(work):
                entities.append({'name': work, 'type': 'WORK'})
                seen.add(work.lower())
        
        # Extract parenthetical works (TV shows, movies) - more comprehensive
        paren_works = re.findall(r'\(([^)]+)\)', text)
        for work in paren_works:
            work = work.strip()
            if (2 < len(work) < 50 and 
                work.lower() not in seen and 
                not self._is_malformed_entity(work)):
                # Check if it's likely a work vs person name
                # Works often have: The, multiple words, or known patterns
                if (work.startswith('The ') or 
                    work.startswith('V/H/S') or 
                    len(work.split()) > 2 or
                    any(word in work.lower() for word in ['dead', 'wolf', 'caretaker', 'act', 'horror'])):
                    entities.append({'name': work, 'type': 'WORK'})
                    seen.add(work.lower())
        
        # Person patterns - strict and clean
        person_patterns = [
            # Full names: First Last or First Middle Last (avoid action phrases)
            r'\b([A-Z][a-z]{2,}(?:\s+[A-Z][a-z]{1,2}\.?)?\s+[A-Z][a-z]{2,})\b(?!\s+(?:announces|says|stated|joined|stars|appears|filming))',
            # Titled persons - extract name only, not title
            r'\b(?:Mayor|Judge|Commissioner|Rep\.|Representative|Senator|Dr\.|President)\s+([A-Z][a-z]{2,}(?:\s+[A-Z][a-z]{2,})*)\b',
        ]
        
        for pattern in person_patterns:
            for match in re.finditer(pattern, text):
                name = match.group(1).strip()
                
                # Skip if this looks like a malformed entity
                if self._is_malformed_entity(name):
                    continue
                    
                # Filter out common false positives and action phrases
                if (name and len(name) > 3 and 
                    name.lower() not in seen and
                    not any(word in name.lower() for word in [
                        'service', 'county', 'office', 'department', 'road', 'street', 
                        'announces', 'announced', 'underway', 'producer', 'actress',
                        'horror', 'filming', 'project', 'infrastructure', 'paranormal'
                    ]) and
                    not re.search(r'\b(?:announces|joined|stars|appears|filming|from|with)\b', name.lower())):
                    entities.append({'name': name, 'type': 'PERSON'})
                    seen.add(name.lower())
        
        # Location patterns - more specific and clean
        location_patterns = [
            r'\b(Scranton)\b',
            r'\b(Pennsylvania|PA)\b',
            r'\b(Lackawanna County)\b',
            # Street addresses - more precise
            r'\b([A-Z][a-z]+(?:\s+[A-Z][a-z]+){0,2}\s+(?:Road|Street|Avenue|Drive|Lane|Boulevard|Highway))\b',
            # Venues - specific patterns
            r'\b(Ritz Theater|City Hall)\b',
            r'\b([A-Z][a-z]+\s+(?:Theater|Theatre|Hall|Park|Lake|River))\b(?!\s+and)',
            # Venues mentioned in filming context  
            r'\bfilming at the ([A-Z][a-z]+\s+(?:Theater|Theatre|Hall))\b',
        ]
        
        for pattern in location_patterns:
            for match in re.finditer(pattern, text, re.IGNORECASE):
                name = match.group(1).strip()
                
                # Skip malformed location entities
                if (self._is_malformed_entity(name) or 
                    len(name.split()) > 6 or  # Too long
                    any(word in name.lower() for word in ['million', 'project', 'improvement', 'targeting'])):
                    continue
                    
                if name and name.lower() not in seen:
                    # Normalize Pennsylvania/PA
                    if name.upper() == 'PA':
                        name = 'Pennsylvania'
                    entities.append({'name': name, 'type': 'LOCATION'})
                    seen.add(name.lower())
        
        # Organization patterns
        org_patterns = [
            # Government agencies
            r'\b([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\s+(?:Department|Office|Service|Agency|Bureau|Administration))\b',
            # Companies
            r'\b([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\s+(?:Company|Corporation|Inc\.|LLC|Corp\.))\b',
            # Universities/Schools
            r'\b([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\s+(?:University|College|School))\b',
            # News organizations
            r'\b(CNN|NBC|ABC|CBS|FOX|NPR|WBRE|WYOU)\b',
            # Specific organizations
            r'\b(National Weather Service|NWS)\b',
            r'\b(FBI|CIA|NSA|EPA|FDA)\b',
        ]
        
        for pattern in org_patterns:
            for match in re.finditer(pattern, text):
                name = match.group(1)
                if name and name.lower() not in seen and len(name) > 2:
                    entities.append({'name': name, 'type': 'ORGANIZATION'})
                    seen.add(name.lower())
        
        # Work/Media patterns - movies, TV shows, books, projects
        work_patterns = [
            r'\b(Final Act|V/H/S|Teen Wolf|The Walking Dead|The Caretaker)\b',
            r'\b([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s+(?:\(film\)|\(movie\)|\(TV series\)|\(book\))\b',
            r'\bindependent film ([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\b',
            r'\bmovie ([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\b',
            r'\bTV series ([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\b',
            r'\bhorror-thriller ([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\b',
        ]
        
        for pattern in work_patterns:
            for match in re.finditer(pattern, text, re.IGNORECASE):
                name = match.group(1) if len(match.groups()) > 0 else match.group(0)
                if name and name.lower() not in seen and not self._is_malformed_entity(name):
                    entities.append({'name': name, 'type': 'WORK'})
                    seen.add(name.lower())

        # Event patterns
        event_patterns = [
            r'\b([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\s+(?:Festival|Celebration|Conference|Summit|Game|Tournament))\b',
            r'\b(Pride Month|Memorial Day|Labor Day|Independence Day)\b',
            r'\b(Flash Flood Warning|Flash Flood)\b',
            r'\b([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\s+(?:Warning|Alert|Advisory|Emergency))\b',
            r'\b(Weather Warning|Storm Warning|Heat Advisory)\b',
        ]
        
        for pattern in event_patterns:
            for match in re.finditer(pattern, text, re.IGNORECASE):
                name = match.group(1)
                if name and name.lower() not in seen:
                    entities.append({'name': name, 'type': 'EVENT'})
                    seen.add(name.lower())
        
        # Always include Scranton if not present
        if 'scranton' not in seen and 'Scranton' in text:
            entities.append({'name': 'Scranton', 'type': 'LOCATION'})
        
        return entities[:10]  # Limit to 10
    
    def _extract_relationships_with_fallback(self, text: str, entities: List[Dict]) -> List[Dict]:
        """Extract relationships using best available method."""
        
        # Try Ollama for relationship extraction
        if self.ollama_available and entities:
            try:
                relationships = self._ollama_extract_relationships(text, entities)
                if relationships:
                    return relationships
            except Exception as e:
                print(f"Ollama relationship extraction failed: {e}")
        
        # Fallback to rule-based relationships
        return self._rule_based_relationships(text, entities)
    
    def _ollama_extract_relationships(self, text: str, entities: List[Dict]) -> List[Dict]:
        """Extract relationships using Ollama."""
        import ollama
        
        entity_names = [e['name'] for e in entities]
        prompt = f"""Given entities: {entity_names}

Extract ALL verb-based relationships from this text. Every verb between entities should become an edge.
Return ONLY valid JSON array with 'from', 'to', 'type', 'verb' fields.

Examples:
- "Mayor Smith announced the project" ‚Üí {{"from": "Smith", "to": "project", "type": "ANNOUNCED", "verb": "announced"}}
- "John works for Apple" ‚Üí {{"from": "John", "to": "Apple", "type": "WORKS_FOR", "verb": "works"}}
- "Company bought building" ‚Üí {{"from": "Company", "to": "building", "type": "PURCHASED", "verb": "bought"}}

Text: {text}

JSON:"""
        
        response = ollama.generate(
            model=self.ollama_model,
            prompt=prompt,
            options={"temperature": 0.1, "num_predict": 150}
        )
        
        response_text = response['response']
        json_match = re.search(r'\[.*?\]', response_text, re.DOTALL)
        
        if json_match:
            try:
                relationships = json.loads(json_match.group(0))
                return self._validate_relationships(relationships, entities)
            except json.JSONDecodeError:
                return []
        
        return []
    
    def _rule_based_relationships(self, text: str, entities: List[Dict]) -> List[Dict]:
        """Enhanced rule-based relationship extraction using comprehensive verb patterns."""
        relationships = []
        entity_lookup = {}
        
        # Create flexible entity lookup (partial matching)
        for entity in entities:
            name = entity['name']
            entity_lookup[name.lower()] = entity
            # Also add individual words for partial matching
            words = name.split()
            if len(words) > 1:
                for word in words:
                    if len(word) > 3:  # Only meaningful words
                        entity_lookup[word.lower()] = entity
        
        # First, extract ALL verb-based relationships
        relationships.extend(self._extract_all_verb_relationships(text, entities, entity_lookup))
        
        # Enhanced relationship patterns with more comprehensive coverage
        patterns = [
            # Professional relationships
            (r'(Mayor|Judge|Commissioner|Rep\.|Representative|Senator|Dr\.|President)\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)', 'HAS_TITLE'),
            (r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s+(?:announced|declared|stated|said)', 'ANNOUNCED'),
            (r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s+(?:from|of)\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)', 'MEMBER_OF'),
            
            # Location relationships
            (r'(?:in|at|located\s+in|based\s+in)\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)', 'LOCATED_IN'),
            (r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s+(?:in|at)\s+(Scranton|Pennsylvania)', 'LOCATED_IN'),
            (r'(filming|shooting|recorded)\s+(?:in|at)\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)', 'OCCURS_AT'),
            
            # Work/participation relationships
            (r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s+(?:joined|stars\s+in|appears\s+in|cast\s+in)\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)', 'PARTICIPATES_IN'),
            (r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s+(?:directed|produced|created)\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)', 'CREATED'),
            (r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s+(?:plays|portrays|acts\s+as)\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)', 'PORTRAYS'),
            
            # Event relationships
            (r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s+(?:during|for)\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)', 'OCCURS_DURING'),
            (r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s+(?:issued|released)\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)', 'ISSUED'),
            
            # Legal/administrative relationships
            (r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s+(?:sentenced|charged|arrested)', 'LEGAL_ACTION'),
            (r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s+(?:sued|filed\s+lawsuit\s+against)\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)', 'LEGAL_DISPUTE'),
            
            # Educational/sports relationships
            (r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s+(?:chooses|commits\s+to|signed\s+with)\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)', 'COMMITTED_TO'),
            (r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s+(?:named\s+to|selected\s+for)\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)', 'SELECTED_FOR'),
        ]
        
        for pattern, rel_type in patterns:
            for match in re.finditer(pattern, text, re.IGNORECASE):
                groups = match.groups()
                if len(groups) >= 2:
                    subj_text = groups[0].strip()
                    obj_text = groups[1].strip()
                    
                    # Find matching entities (exact or partial)
                    subj_entity = self._find_matching_entity(subj_text, entity_lookup)
                    obj_entity = self._find_matching_entity(obj_text, entity_lookup)
                    
                    if subj_entity and obj_entity and subj_entity['name'] != obj_entity['name']:
                        relationships.append({
                            'from': subj_entity['name'],
                            'to': obj_entity['name'],
                            'type': rel_type
                        })
                
                # Handle single-group patterns (like ANNOUNCED)
                elif len(groups) == 1 and rel_type in ['ANNOUNCED', 'LEGAL_ACTION']:
                    subj_text = groups[0].strip()
                    subj_entity = self._find_matching_entity(subj_text, entity_lookup)
                    
                    # Connect to Scranton for local context
                    scranton_entity = next((e for e in entities if e['name'] == 'Scranton'), None)
                    if subj_entity and scranton_entity and subj_entity['name'] != 'Scranton':
                        relationships.append({
                            'from': subj_entity['name'],
                            'to': 'Scranton',
                            'type': 'RELATES_TO'
                        })
        
        # Add geographic relationships
        for entity in entities:
            if entity['type'] == 'LOCATION' and entity['name'] != 'Scranton':
                scranton_entity = next((e for e in entities if e['name'] == 'Scranton'), None)
                if scranton_entity:
                    relationships.append({
                        'from': entity['name'],
                        'to': 'Scranton',
                        'type': 'LOCATED_NEAR'
                    })
        
        # Add organizational relationships to Scranton
        for entity in entities:
            if entity['type'] == 'ORGANIZATION' and 'scranton' in text.lower():
                scranton_entity = next((e for e in entities if e['name'] == 'Scranton'), None)
                if scranton_entity and entity['name'] != 'Scranton':
                    relationships.append({
                        'from': entity['name'],
                        'to': 'Scranton',
                        'type': 'OPERATES_IN'
                    })
        
        # Remove duplicates
        unique_relationships = []
        seen = set()
        for rel in relationships:
            key = (rel['from'], rel['to'], rel['type'])
            if key not in seen:
                seen.add(key)
                unique_relationships.append(rel)
        
        return unique_relationships[:8]  # Limit for readability
    
    def _is_malformed_entity(self, name: str) -> bool:
        """Check if an entity name looks malformed or like a concatenated phrase."""
        name_lower = name.lower()
        
        # Contains action verbs that indicate it's a headline/phrase
        action_verbs = [
            'announces', 'announced', 'says', 'said', 'states', 'stated',
            'joined', 'stars', 'appears', 'filming', 'underway', 'from'
        ]
        
        if any(verb in name_lower for verb in action_verbs):
            return True
            
        # Too many words (likely a phrase)
        if len(name.split()) > 4:
            return True
            
        # Mixed case with function words (likely a phrase)
        function_words = ['the', 'with', 'from', 'and', 'or', 'in', 'at', 'on']
        words = name_lower.split()
        if len(words) > 2 and any(word in function_words for word in words):
            return True
            
        # Contains numbers and words (likely not a clean entity)
        if re.search(r'\d.*[a-z]|[a-z].*\d', name_lower):
            return True
            
        return False
    
    def _extract_all_verb_relationships(self, text: str, entities: List[Dict], entity_lookup: Dict) -> List[Dict]:
        """Extract relationships using ALL verbs as potential edges."""
        relationships = []
        
        # Common English verbs that indicate relationships
        relationship_verbs = {
            # Action verbs
            'announced', 'said', 'stated', 'declared', 'reported', 'confirmed', 'revealed',
            'joined', 'left', 'quit', 'hired', 'fired', 'promoted', 'demoted',
            'created', 'built', 'designed', 'developed', 'launched', 'started', 'founded',
            'bought', 'sold', 'acquired', 'purchased', 'invested', 'funded', 'sponsored',
            'opened', 'closed', 'moved', 'relocated', 'expanded', 'reduced', 'increased',
            'won', 'lost', 'defeated', 'beat', 'competed', 'participated', 'attended',
            'married', 'divorced', 'dated', 'met', 'knew', 'befriended', 'partnered',
            'sued', 'charged', 'arrested', 'sentenced', 'convicted', 'accused', 'blamed',
            'elected', 'appointed', 'nominated', 'selected', 'chosen', 'picked',
            'visited', 'traveled', 'went', 'came', 'arrived', 'departed', 'returned',
            'supports', 'opposes', 'endorses', 'backs', 'promotes', 'advocates',
            'leads', 'manages', 'directs', 'supervises', 'oversees', 'heads',
            'owns', 'operates', 'runs', 'controls', 'manages', 'administers',
            'teaches', 'learns', 'studies', 'graduates', 'enrolls', 'attends',
            'works', 'serves', 'volunteers', 'helps', 'assists', 'aids',
            'lives', 'resides', 'stays', 'inhabits', 'occupies', 'dwells',
            'plays', 'performs', 'acts', 'stars', 'appears', 'features',
            'writes', 'authors', 'publishes', 'edits', 'reviews', 'critiques',
            'produces', 'directs', 'films', 'shoots', 'records', 'broadcasts'
        }
        
        # Pattern: Entity + Verb + Entity
        entity_names = [e['name'] for e in entities]
        for i, entity1 in enumerate(entities):
            for j, entity2 in enumerate(entities):
                if i >= j:  # Avoid duplicates and self-references
                    continue
                
                # Look for patterns: Entity1 verb Entity2
                for verb in relationship_verbs:
                    patterns = [
                        # Direct patterns
                        rf'\b{re.escape(entity1["name"])}\b[^.!?]*?\b{verb}s?\b[^.!?]*?\b{re.escape(entity2["name"])}\b',
                        rf'\b{re.escape(entity2["name"])}\b[^.!?]*?\b{verb}s?\b[^.!?]*?\b{re.escape(entity1["name"])}\b',
                        # Passive patterns  
                        rf'\b{re.escape(entity1["name"])}\b[^.!?]*?\bwas {verb}[ed|n]?\b[^.!?]*?\b{re.escape(entity2["name"])}\b',
                        rf'\b{re.escape(entity2["name"])}\b[^.!?]*?\bwas {verb}[ed|n]?\b[^.!?]*?\b{re.escape(entity1["name"])}\b',
                    ]
                    
                    for pattern in patterns:
                        if re.search(pattern, text, re.IGNORECASE):
                            # Determine direction based on pattern match
                            if re.search(rf'\b{re.escape(entity1["name"])}\b[^.!?]*?\b{verb}', text, re.IGNORECASE):
                                from_entity, to_entity = entity1['name'], entity2['name']
                            else:
                                from_entity, to_entity = entity2['name'], entity1['name']
                            
                            rel_type = self._normalize_verb_to_relationship(verb)
                            relationships.append({
                                'from': from_entity,
                                'to': to_entity,
                                'type': rel_type,
                                'verb': verb
                            })
                            break  # Found relationship, move to next verb
        
        return relationships
    
    def _normalize_verb_to_relationship(self, verb: str) -> str:
        """Convert verbs to normalized relationship types."""
        verb_mappings = {
            # Communication
            'announced': 'ANNOUNCED', 'said': 'SAID', 'stated': 'STATED', 'declared': 'DECLARED',
            'reported': 'REPORTED', 'confirmed': 'CONFIRMED', 'revealed': 'REVEALED',
            
            # Employment/Organization  
            'joined': 'JOINED', 'left': 'LEFT', 'hired': 'HIRED', 'fired': 'FIRED',
            'works': 'WORKS_FOR', 'serves': 'SERVES', 'leads': 'LEADS', 'manages': 'MANAGES',
            
            # Creation/Development
            'created': 'CREATED', 'built': 'BUILT', 'designed': 'DESIGNED', 'founded': 'FOUNDED',
            'developed': 'DEVELOPED', 'launched': 'LAUNCHED', 'started': 'STARTED',
            
            # Business/Finance
            'bought': 'PURCHASED', 'sold': 'SOLD', 'acquired': 'ACQUIRED', 'invested': 'INVESTED',
            'funded': 'FUNDED', 'sponsored': 'SPONSORED', 'owns': 'OWNS',
            
            # Location/Movement
            'moved': 'MOVED_TO', 'relocated': 'RELOCATED_TO', 'visited': 'VISITED',
            'lives': 'LIVES_IN', 'resides': 'RESIDES_IN', 'traveled': 'TRAVELED_TO',
            
            # Legal/Government
            'sued': 'SUED', 'charged': 'CHARGED', 'arrested': 'ARRESTED', 'elected': 'ELECTED',
            'appointed': 'APPOINTED', 'nominated': 'NOMINATED', 'sentenced': 'SENTENCED',
            
            # Social/Personal
            'married': 'MARRIED', 'met': 'MET', 'knew': 'KNEW', 'befriended': 'BEFRIENDED',
            'supports': 'SUPPORTS', 'opposes': 'OPPOSES', 'endorses': 'ENDORSES',
            
            # Performance/Arts
            'plays': 'PLAYS', 'performs': 'PERFORMS', 'stars': 'STARS_IN', 'appears': 'APPEARS_IN',
            'wrote': 'WROTE', 'produced': 'PRODUCED', 'directed': 'DIRECTED',
            
            # Competition/Achievement
            'won': 'WON', 'defeated': 'DEFEATED', 'competed': 'COMPETED_AGAINST',
            'participated': 'PARTICIPATED_IN', 'graduated': 'GRADUATED_FROM'
        }
        
        return verb_mappings.get(verb.lower(), verb.upper().replace(' ', '_'))
    
    def _find_matching_entity(self, text: str, entity_lookup: Dict) -> Optional[Dict]:
        """Find matching entity using exact or partial matching."""
        text_lower = text.lower()
        
        # Try exact match first
        if text_lower in entity_lookup:
            return entity_lookup[text_lower]
        
        # Try partial matching - find entity that contains this text or vice versa
        for key, entity in entity_lookup.items():
            if (text_lower in key or key in text_lower) and len(key) > 3:
                return entity
        
        return None
    
    def _validate_entities(self, entities: List[Dict]) -> List[Dict]:
        """Validate and clean entities."""
        valid_entities = []
        valid_types = {'PERSON', 'ORGANIZATION', 'LOCATION', 'DATE', 'EVENT', 'WORK', 'OTHER'}
        
        for entity in entities:
            if (isinstance(entity, dict) and 
                'name' in entity and 
                'type' in entity and
                entity['type'] in valid_types and
                len(entity['name']) > 2):
                valid_entities.append(entity)
        
        return self._deduplicate_entities(valid_entities)
    
    def _validate_relationships(self, relationships: List[Dict], entities: List[Dict]) -> List[Dict]:
        """Validate relationships against entities."""
        entity_names = {e['name'] for e in entities}
        valid_relationships = []
        
        for rel in relationships:
            if (isinstance(rel, dict) and 
                'from' in rel and 
                'to' in rel and 
                'type' in rel and
                rel['from'] in entity_names and
                rel['to'] in entity_names):
                valid_relationships.append(rel)
        
        return valid_relationships
    
    def _deduplicate_entities(self, entities: List[Dict]) -> List[Dict]:
        """Remove duplicate entities."""
        seen_names = set()
        unique_entities = []
        
        for entity in entities:
            name = entity['name']
            if name not in seen_names and len(name) > 2:
                unique_entities.append(entity)
                seen_names.add(name)
        
        return unique_entities
    
    def _generate_svg_graph(self, entities: List[Dict], relationships: List[Dict], index: int) -> str:
        """Generate SVG visualization of the knowledge graph."""
        if not entities:
            return '<svg width="300" height="200"><text x="150" y="100" text-anchor="middle" fill="white">No entities found</text></svg>'
        
        colors = {
            'PERSON': '#FF6B6B',
            'ORGANIZATION': '#4ECDC4',
            'LOCATION': '#45B7D1',
            'DATE': '#FFA726',
            'EVENT': '#AB47BC',
            'WORK': '#9C27B0',
            'OTHER': '#78909C'
        }
        
        svg = '<svg width="300" height="200" viewBox="0 0 300 200" class="graph-svg">'
        
        # Draw entities as circles
        for i, entity in enumerate(entities[:6]):  # Limit to 6 for layout
            x = 50 + (i % 3) * 100
            y = 50 + (i // 3) * 60
            color = colors.get(entity['type'], '#888')
            
            svg += f'<circle cx="{x}" cy="{y}" r="20" fill="{color}" stroke="white" stroke-width="2"/>'
            svg += f'<text x="{x}" y="{y-25}" text-anchor="middle" fill="white" font-size="10" font-weight="bold">{entity["name"][:8]}</text>'
            svg += f'<text x="{x}" y="{y+35}" text-anchor="middle" fill="white" font-size="8">{entity["type"]}</text>'
        
        # Draw relationships as lines
        for rel in relationships:
            from_entity = next((e for e in entities if e['name'] == rel['from']), None)
            to_entity = next((e for e in entities if e['name'] == rel['to']), None)
            
            if from_entity and to_entity:
                from_index = entities.index(from_entity)
                to_index = entities.index(to_entity)
                
                if from_index < 6 and to_index < 6:  # Only draw if both visible
                    x1 = 50 + (from_index % 3) * 100
                    y1 = 50 + (from_index // 3) * 60
                    x2 = 50 + (to_index % 3) * 100
                    y2 = 50 + (to_index // 3) * 60
                    
                    svg += f'<line x1="{x1}" y1="{y1}" x2="{x2}" y2="{y2}" stroke="white" stroke-width="1" opacity="0.7"/>'
                    
                    # Add relationship label
                    mid_x = (x1 + x2) // 2
                    mid_y = (y1 + y2) // 2
                    svg += f'<text x="{mid_x}" y="{mid_y}" text-anchor="middle" fill="white" font-size="8">{rel["type"]}</text>'
        
        svg += '</svg>'
        return svg
    
    def _calculate_confidence(self, entities: List[Dict]) -> float:
        """Calculate overall confidence score."""
        if not entities:
            return 0.0
        
        confidences = [e.get('confidence', 0.5) for e in entities]
        return sum(confidences) / len(confidences)
    
    def _get_active_method(self) -> str:
        """Get the active extraction method."""
        if self.ollama_available:
            return f"ollama_{self.ollama_model}"
        elif self.hf_available:
            return "huggingface_bert"
        else:
            return "rule_based"
    
    def get_method_info(self) -> Dict:
        """Get information about available methods."""
        return {
            "ollama_available": self.ollama_available,
            "ollama_model": self.ollama_model if self.ollama_available else None,
            "huggingface_available": self.hf_available,
            "active_method": self._get_active_method(),
            "cost_per_article": "$0.00",  # Free!
            "privacy_preserving": self.ollama_available or self.hf_available
        }


# Integration function for generate_shorts.py
def create_free_llm_graph_data(article: Dict, index: int = 0) -> Dict:
    """
    Drop-in replacement for generate_article_graph() in generate_shorts.py
    Enhanced with Obsidian entity vault integration.
    """
    extractor = ProductionFreeLLMExtractor()
    result = extractor.extract_for_article(article, index)
    
    # Integrate with Obsidian entity vault
    try:
        from obsidian_entity_manager import integrate_with_obsidian
        
        obsidian_result = integrate_with_obsidian(
            result['entities'], 
            result['relationships']
        )
        
        # Update result with resolved entities
        result['entities'] = obsidian_result['resolved_entities']
        result['vault_statistics'] = obsidian_result['vault_statistics']
        result['entity_files'] = obsidian_result['entity_files']
        result['method'] = f"{result['method']}_with_obsidian"
        
        print(f"‚úÖ Integrated with Obsidian vault: {len(obsidian_result['resolved_entities'])} entities")
        
    except ImportError:
        print("‚ö†Ô∏è Obsidian integration not available, using standard extraction")
    except Exception as e:
        print(f"‚ö†Ô∏è Obsidian integration failed: {e}, using standard extraction")
    
    return result


if __name__ == "__main__":
    # Test the extractor with the movie article example from EXTRACTION_PATTERNS.md
    test_article = {
        "title": "'Final Act': Paranormal Horror From 'Popeye The Slayer Man' Producer Underway With Avaryana Rose, Hannah Fierman, Douglas Tait & Vincent M. Ward",
        "description": "Actress and influencer Avaryana Rose (The Caretaker) has joined Hannah Fierman (V/H/S), Douglas Tait (Teen Wolf), and Vincent M. Ward (The Walking Dead) in indie horror-thriller Final Act, which is filming at the Ritz Theater and other locations in Scranton."
    }
    
    extractor = ProductionFreeLLMExtractor()
    result = extractor.extract_for_article(test_article)
    
    print("üî¨ Free LLM Extractor Test Results (Verb-Enhanced):")
    print(f"Method: {result['method']}")
    print(f"Entities ({len(result['entities'])}):")
    for entity in result['entities']:
        print(f"  - {entity['name']} ({entity['type']})")
    
    print(f"\nVerb-based Relationships ({len(result['relationships'])}):")
    for rel in result['relationships']:
        verb_info = f" [verb: {rel['verb']}]" if 'verb' in rel else ""
        print(f"  - {rel['from']} ‚Üí {rel['to']} ({rel['type']}){verb_info}")
    
    print(f"\nConfidence: {result['confidence']:.2f}")
    print(f"Method Info: {extractor.get_method_info()}")
    print("\n‚úÖ All verbs should now appear as graph edges!")