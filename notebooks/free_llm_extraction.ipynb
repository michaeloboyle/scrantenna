{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Free LLM Models for Entity and Relationship Extraction\n",
    "\n",
    "This notebook explores free/open-source LLM alternatives to OpenAI for entity and relationship extraction in news articles.\n",
    "\n",
    "## Model Options Evaluated:\n",
    "1. **Ollama** (Local inference)\n",
    "   - Llama 2/3 variants\n",
    "   - Mistral models\n",
    "   - CodeLlama for structured output\n",
    "\n",
    "2. **Hugging Face Transformers** (Local/Cloud)\n",
    "   - BERT-based NER models\n",
    "   - GPT-2/GPT-Neo variants\n",
    "   - Specialized entity extraction models\n",
    "\n",
    "3. **Local Inference Servers**\n",
    "   - llama.cpp\n",
    "   - text-generation-webui\n",
    "   - vLLM for faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import re\n",
    "from typing import List, Dict, Optional\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Try to import optional dependencies\n",
    "try:\n",
    "    import ollama\n",
    "    ollama_available = True\n",
    "except ImportError:\n",
    "    print(\"Ollama not available. Install with: pip install ollama\")\n",
    "    ollama_available = False\n",
    "\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "    transformers_available = True\n",
    "except ImportError:\n",
    "    print(\"Transformers not available. Install with: pip install transformers torch\")\n",
    "    transformers_available = False\n",
    "\n",
    "# Load sample news data\n",
    "def load_sample_news():\n",
    "    \"\"\"Load sample Scranton news for testing.\"\"\"\n",
    "    try:\n",
    "        with open('../data/daily/scranton_news_2025-06-25.json', 'r') as f:\n",
    "            news_data = json.load(f)\n",
    "        return news_data.get('articles', [])[:3]  # Test with first 3 articles\n",
    "    except FileNotFoundError:\n",
    "        # Fallback sample data\n",
    "        return [\n",
    "            {\n",
    "                \"title\": \"Mayor Cognetti Announces New Infrastructure Project in Scranton\",\n",
    "                \"description\": \"Scranton Mayor Paige Cognetti announced a $2 million infrastructure improvement project targeting Providence Road. The project will improve drainage and road conditions for residents.\"\n",
    "            }\n",
    "        ]\n",
    "\n",
    "sample_articles = load_sample_news()\n",
    "print(f\"Loaded {len(sample_articles)} sample articles for testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ollama Local LLM Integration\n",
    "\n",
    "Ollama provides easy local inference for various open-source models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaExtractor:\n",
    "    \"\"\"Entity and relationship extraction using Ollama.\"\"\"\n",
    "    \n",
    "    def __init__(self, model=\"llama3.2:3b\"):\n",
    "        self.model = model\n",
    "        self.available = ollama_available\n",
    "        \n",
    "        if self.available:\n",
    "            try:\n",
    "                # Test if model is available\n",
    "                ollama.show(model)\n",
    "                print(f\"‚úì Ollama model {model} is available\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Ollama model {model} not found. Pull with: ollama pull {model}\")\n",
    "                self.available = False\n",
    "    \n",
    "    def extract_entities(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Extract named entities using Ollama.\"\"\"\n",
    "        if not self.available:\n",
    "            return []\n",
    "        \n",
    "        prompt = f\"\"\"Extract named entities from this news text. Return only a JSON array of entities with 'name' and 'type' fields. Types should be: PERSON, ORGANIZATION, LOCATION, DATE, EVENT.\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "JSON Response:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = ollama.generate(\n",
    "                model=self.model,\n",
    "                prompt=prompt,\n",
    "                options={\n",
    "                    \"temperature\": 0.1,\n",
    "                    \"top_p\": 0.9,\n",
    "                    \"num_predict\": 200\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # Extract JSON from response\n",
    "            response_text = response['response']\n",
    "            json_match = re.search(r'\\[.*\\]', response_text, re.DOTALL)\n",
    "            \n",
    "            if json_match:\n",
    "                entities_json = json_match.group(0)\n",
    "                entities = json.loads(entities_json)\n",
    "                return entities\n",
    "            else:\n",
    "                print(f\"No JSON found in response: {response_text}\")\n",
    "                return []\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Ollama extraction failed: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def extract_relationships(self, text: str, entities: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Extract relationships between entities.\"\"\"\n",
    "        if not self.available or not entities:\n",
    "            return []\n",
    "        \n",
    "        entity_names = [e['name'] for e in entities]\n",
    "        prompt = f\"\"\"Given these entities: {entity_names}\n",
    "        \n",
    "Extract relationships from this text. Return JSON array with 'from', 'to', 'type' fields. Relationship types: LOCATED_IN, WORKS_FOR, ANNOUNCED, PARTICIPATES_IN, PART_OF.\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "JSON Response:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = ollama.generate(\n",
    "                model=self.model,\n",
    "                prompt=prompt,\n",
    "                options={\"temperature\": 0.1, \"num_predict\": 150}\n",
    "            )\n",
    "            \n",
    "            response_text = response['response']\n",
    "            json_match = re.search(r'\\[.*\\]', response_text, re.DOTALL)\n",
    "            \n",
    "            if json_match:\n",
    "                relationships = json.loads(json_match.group(0))\n",
    "                return relationships\n",
    "            return []\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Ollama relationship extraction failed: {e}\")\n",
    "            return []\n",
    "\n",
    "# Test Ollama extraction\n",
    "ollama_extractor = OllamaExtractor()\n",
    "\n",
    "if ollama_extractor.available and sample_articles:\n",
    "    test_text = f\"{sample_articles[0]['title']} {sample_articles[0]['description']}\"\n",
    "    print(f\"\\nTesting Ollama extraction on: {test_text[:100]}...\")\n",
    "    \n",
    "    entities = ollama_extractor.extract_entities(test_text)\n",
    "    print(f\"Entities found: {entities}\")\n",
    "    \n",
    "    relationships = ollama_extractor.extract_relationships(test_text, entities)\n",
    "    print(f\"Relationships found: {relationships}\")\nelse:\n",
    "    print(\"Ollama not available for testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hugging Face Transformers Integration\n",
    "\n",
    "Using specialized NER models from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuggingFaceExtractor:\n",
    "    \"\"\"Entity extraction using Hugging Face transformers.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.available = transformers_available\n",
    "        self.ner_pipeline = None\n",
    "        \n",
    "        if self.available:\n",
    "            try:\n",
    "                # Use a lightweight NER model\n",
    "                self.ner_pipeline = pipeline(\n",
    "                    \"ner\", \n",
    "                    model=\"dbmdz/bert-large-cased-finetuned-conll03-english\",\n",
    "                    aggregation_strategy=\"simple\"\n",
    "                )\n",
    "                print(\"‚úì Hugging Face NER pipeline loaded\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Failed to load HF model: {e}\")\n",
    "                self.available = False\n",
    "    \n",
    "    def extract_entities(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Extract entities using Hugging Face NER.\"\"\"\n",
    "        if not self.available or not self.ner_pipeline:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # Run NER pipeline\n",
    "            ner_results = self.ner_pipeline(text)\n",
    "            \n",
    "            # Convert to our format\n",
    "            entities = []\n",
    "            for result in ner_results:\n",
    "                entity_type = self.map_entity_type(result['entity_group'])\n",
    "                entities.append({\n",
    "                    'name': result['word'],\n",
    "                    'type': entity_type,\n",
    "                    'confidence': result['score']\n",
    "                })\n",
    "            \n",
    "            # Remove duplicates and low confidence entities\n",
    "            filtered_entities = []\n",
    "            seen_names = set()\n",
    "            \n",
    "            for entity in entities:\n",
    "                if (entity['confidence'] > 0.7 and \n",
    "                    entity['name'] not in seen_names and\n",
    "                    len(entity['name']) > 2):\n",
    "                    filtered_entities.append(entity)\n",
    "                    seen_names.add(entity['name'])\n",
    "            \n",
    "            return filtered_entities\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"HuggingFace extraction failed: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def map_entity_type(self, hf_type: str) -> str:\n",
    "        \"\"\"Map HuggingFace entity types to our schema.\"\"\"\n",
    "        mapping = {\n",
    "            'PER': 'PERSON',\n",
    "            'ORG': 'ORGANIZATION', \n",
    "            'LOC': 'LOCATION',\n",
    "            'MISC': 'OTHER'\n",
    "        }\n",
    "        return mapping.get(hf_type, 'OTHER')\n",
    "    \n",
    "    def extract_relationships(self, text: str, entities: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Simple rule-based relationship extraction.\"\"\"\n",
    "        relationships = []\n",
    "        \n",
    "        # Simple patterns for relationship detection\n",
    "        patterns = [\n",
    "            (r'(\\w+)\\s+announced', 'ANNOUNCED'),\n",
    "            (r'(\\w+)\\s+in\\s+(\\w+)', 'LOCATED_IN'),\n",
    "            (r'Mayor\\s+(\\w+)', 'TITLE_OF')\n",
    "        ]\n",
    "        \n",
    "        for pattern, rel_type in patterns:\n",
    "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                if len(match.groups()) >= 2:\n",
    "                    subj = match.group(1)\n",
    "                    obj = match.group(2)\n",
    "                    \n",
    "                    # Check if entities exist\n",
    "                    if any(e['name'].lower() == subj.lower() for e in entities):\n",
    "                        if any(e['name'].lower() == obj.lower() for e in entities):\n",
    "                            relationships.append({\n",
    "                                'from': subj,\n",
    "                                'to': obj,\n",
    "                                'type': rel_type\n",
    "                            })\n",
    "        \n",
    "        return relationships\n",
    "\n",
    "# Test Hugging Face extraction\n",
    "hf_extractor = HuggingFaceExtractor()\n",
    "\n",
    "if hf_extractor.available and sample_articles:\n",
    "    test_text = f\"{sample_articles[0]['title']} {sample_articles[0]['description']}\"\n",
    "    print(f\"\\nTesting HuggingFace extraction on: {test_text[:100]}...\")\n",
    "    \n",
    "    entities = hf_extractor.extract_entities(test_text)\n",
    "    print(f\"Entities found: {entities}\")\n",
    "    \n",
    "    relationships = hf_extractor.extract_relationships(test_text, entities)\n",
    "    print(f\"Relationships found: {relationships}\")\nelse:\n",
    "    print(\"HuggingFace not available for testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Free API Services Integration\n",
    "\n",
    "Some free cloud-based LLM services with reasonable limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FreeAPIExtractor:\n",
    "    \"\"\"Using free API services for extraction.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Check for API keys in environment\n",
    "        self.huggingface_token = os.getenv('HUGGINGFACE_TOKEN')\n",
    "        self.together_api_key = os.getenv('TOGETHER_API_KEY')\n",
    "        \n",
    "    def huggingface_inference_api(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Use HuggingFace Inference API (free tier).\"\"\"\n",
    "        if not self.huggingface_token:\n",
    "            print(\"HuggingFace token not found. Set HUGGINGFACE_TOKEN env var\")\n",
    "            return []\n",
    "        \n",
    "        # Use free NER endpoint\n",
    "        api_url = \"https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "        headers = {\"Authorization\": f\"Bearer {self.huggingface_token}\"}\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(api_url, headers=headers, json={\"inputs\": text})\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                results = response.json()\n",
    "                entities = []\n",
    "                \n",
    "                for result in results:\n",
    "                    if result['score'] > 0.7:\n",
    "                        entities.append({\n",
    "                            'name': result['word'].replace('##', ''),\n",
    "                            'type': result['entity_group'],\n",
    "                            'confidence': result['score']\n",
    "                        })\n",
    "                \n",
    "                return entities\n",
    "            else:\n",
    "                print(f\"HF API error: {response.status_code}\")\n",
    "                return []\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"HF API extraction failed: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def together_ai_extraction(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Use Together AI free tier for extraction.\"\"\"\n",
    "        if not self.together_api_key:\n",
    "            print(\"Together AI key not found. Set TOGETHER_API_KEY env var\")\n",
    "            return []\n",
    "        \n",
    "        # Together AI offers free credits for open-source models\n",
    "        endpoint = \"https://api.together.xyz/inference\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.together_api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        prompt = f\"\"\"Extract named entities from this news text. Return JSON format only:\n",
    "\n",
    "{text}\n",
    "\n",
    "JSON:\"\"\"\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": \"togethercomputer/llama-2-7b-chat\",\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": 200,\n",
    "            \"temperature\": 0.1\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(endpoint, headers=headers, json=payload)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                generated_text = result['output']['choices'][0]['text']\n",
    "                \n",
    "                # Extract JSON from response\n",
    "                json_match = re.search(r'\\[.*\\]', generated_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    entities = json.loads(json_match.group(0))\n",
    "                    return entities\n",
    "                return []\n",
    "            else:\n",
    "                print(f\"Together AI error: {response.status_code}\")\n",
    "                return []\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Together AI extraction failed: {e}\")\n",
    "            return []\n",
    "\n",
    "# Test free API services\n",
    "api_extractor = FreeAPIExtractor()\n",
    "print(f\"Free API services available:\")\n",
    "print(f\"  HuggingFace: {'‚úì' if api_extractor.huggingface_token else '‚úó'}\")\n",
    "print(f\"  Together AI: {'‚úì' if api_extractor.together_api_key else '‚úó'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comprehensive Fallback Chain Implementation\n",
    "\n",
    "Create a robust extraction system with multiple fallback options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FreeLLMExtractionChain:\n",
    "    \"\"\"Comprehensive entity/relationship extraction with multiple fallbacks.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.extractors = {\n",
    "            'ollama': OllamaExtractor(),\n",
    "            'huggingface': HuggingFaceExtractor(),\n",
    "            'api_services': FreeAPIExtractor()\n",
    "        }\n",
    "        \n",
    "        # Determine available extractors\n",
    "        self.available_extractors = []\n",
    "        for name, extractor in self.extractors.items():\n",
    "            if hasattr(extractor, 'available') and extractor.available:\n",
    "                self.available_extractors.append(name)\n",
    "            elif name == 'api_services':\n",
    "                if extractor.huggingface_token or extractor.together_api_key:\n",
    "                    self.available_extractors.append(name)\n",
    "        \n",
    "        print(f\"Available extractors: {self.available_extractors}\")\n",
    "    \n",
    "    def extract_entities_with_fallback(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Try multiple extraction methods until one succeeds.\"\"\"\n",
    "        \n",
    "        # Try Ollama first (local, fast)\n",
    "        if 'ollama' in self.available_extractors:\n",
    "            try:\n",
    "                entities = self.extractors['ollama'].extract_entities(text)\n",
    "                if entities:\n",
    "                    print(\"‚úì Used Ollama for entity extraction\")\n",
    "                    return entities\n",
    "            except Exception as e:\n",
    "                print(f\"Ollama failed: {e}\")\n",
    "        \n",
    "        # Try HuggingFace transformers (local)\n",
    "        if 'huggingface' in self.available_extractors:\n",
    "            try:\n",
    "                entities = self.extractors['huggingface'].extract_entities(text)\n",
    "                if entities:\n",
    "                    print(\"‚úì Used HuggingFace for entity extraction\")\n",
    "                    return entities\n",
    "            except Exception as e:\n",
    "                print(f\"HuggingFace failed: {e}\")\n",
    "        \n",
    "        # Try free API services\n",
    "        if 'api_services' in self.available_extractors:\n",
    "            api_extractor = self.extractors['api_services']\n",
    "            \n",
    "            if api_extractor.huggingface_token:\n",
    "                try:\n",
    "                    entities = api_extractor.huggingface_inference_api(text)\n",
    "                    if entities:\n",
    "                        print(\"‚úì Used HuggingFace API for entity extraction\")\n",
    "                        return entities\n",
    "                except Exception as e:\n",
    "                    print(f\"HF API failed: {e}\")\n",
    "            \n",
    "            if api_extractor.together_api_key:\n",
    "                try:\n",
    "                    entities = api_extractor.together_ai_extraction(text)\n",
    "                    if entities:\n",
    "                        print(\"‚úì Used Together AI for entity extraction\")\n",
    "                        return entities\n",
    "                except Exception as e:\n",
    "                    print(f\"Together AI failed: {e}\")\n",
    "        \n",
    "        # Final fallback: rule-based extraction\n",
    "        print(\"‚ö†Ô∏è Using rule-based fallback for entity extraction\")\n",
    "        return self.rule_based_entity_extraction(text)\n",
    "    \n",
    "    def rule_based_entity_extraction(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Fallback rule-based entity extraction.\"\"\"\n",
    "        entities = []\n",
    "        \n",
    "        # Simple patterns for entity detection\n",
    "        patterns = {\n",
    "            'PERSON': [r'\\b[A-Z][a-z]+\\s+[A-Z][a-z]+\\b'],\n",
    "            'LOCATION': [r'\\bScranton\\b', r'\\bPennsylvania\\b', r'\\b[A-Z][a-z]+\\s+Road\\b'],\n",
    "            'ORGANIZATION': [r'\\b[A-Z][A-Z]+\\b', r'\\bDepartment\\b', r'\\bOffice\\b']\n",
    "        }\n",
    "        \n",
    "        for entity_type, pattern_list in patterns.items():\n",
    "            for pattern in pattern_list:\n",
    "                matches = re.finditer(pattern, text)\n",
    "                for match in matches:\n",
    "                    entity_name = match.group(0)\n",
    "                    if len(entity_name) > 2 and not any(e['name'] == entity_name for e in entities):\n",
    "                        entities.append({\n",
    "                            'name': entity_name,\n",
    "                            'type': entity_type,\n",
    "                            'method': 'rule_based'\n",
    "                        })\n",
    "        \n",
    "        return entities[:6]  # Limit to top 6\n",
    "    \n",
    "    def extract_relationships_with_fallback(self, text: str, entities: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Extract relationships with fallback methods.\"\"\"\n",
    "        \n",
    "        # Try Ollama first\n",
    "        if 'ollama' in self.available_extractors:\n",
    "            try:\n",
    "                relationships = self.extractors['ollama'].extract_relationships(text, entities)\n",
    "                if relationships:\n",
    "                    print(\"‚úì Used Ollama for relationship extraction\")\n",
    "                    return relationships\n",
    "            except Exception as e:\n",
    "                print(f\"Ollama relationship extraction failed: {e}\")\n",
    "        \n",
    "        # Fallback to rule-based\n",
    "        print(\"‚ö†Ô∏è Using rule-based fallback for relationship extraction\")\n",
    "        return self.rule_based_relationship_extraction(text, entities)\n",
    "    \n",
    "    def rule_based_relationship_extraction(self, text: str, entities: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Rule-based relationship extraction.\"\"\"\n",
    "        relationships = []\n",
    "        entity_names = {e['name'].lower(): e for e in entities}\n",
    "        \n",
    "        # Simple relationship patterns\n",
    "        patterns = [\n",
    "            (r'(\\w+)\\s+announced', 'ANNOUNCED'),\n",
    "            (r'(\\w+)\\s+in\\s+(\\w+)', 'LOCATED_IN'),\n",
    "            (r'Mayor\\s+(\\w+)', 'HAS_TITLE'),\n",
    "            (r'(\\w+)\\s+project.*in\\s+(\\w+)', 'OCCURS_IN')\n",
    "        ]\n",
    "        \n",
    "        for pattern, rel_type in patterns:\n",
    "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                groups = match.groups()\n",
    "                if len(groups) >= 2:\n",
    "                    subj = groups[0].lower()\n",
    "                    obj = groups[1].lower()\n",
    "                    \n",
    "                    if subj in entity_names and obj in entity_names:\n",
    "                        relationships.append({\n",
    "                            'from': entity_names[subj]['name'],\n",
    "                            'to': entity_names[obj]['name'],\n",
    "                            'type': rel_type\n",
    "                        })\n",
    "        \n",
    "        return relationships\n",
    "\n",
    "# Test the complete fallback chain\n",
    "extraction_chain = FreeLLMExtractionChain()\n",
    "\n",
    "if sample_articles:\n",
    "    test_text = f\"{sample_articles[0]['title']} {sample_articles[0]['description']}\"\n",
    "    print(f\"\\nTesting complete extraction chain on: {test_text[:100]}...\\n\")\n",
    "    \n",
    "    # Extract entities with fallback\n",
    "    entities = extraction_chain.extract_entities_with_fallback(test_text)\n",
    "    print(f\"\\nFinal entities: {entities}\")\n",
    "    \n",
    "    # Extract relationships with fallback\n",
    "    relationships = extraction_chain.extract_relationships_with_fallback(test_text, entities)\n",
    "    print(f\"Final relationships: {relationships}\")\n",
    "    \n",
    "    # Performance summary\n",
    "    print(f\"\\nüìä Extraction Summary:\")\n",
    "    print(f\"  Entities found: {len(entities)}\")\n",
    "    print(f\"  Relationships found: {len(relationships)}\")\n",
    "    print(f\"  Available methods: {len(extraction_chain.available_extractors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance and Cost Comparison\n",
    "\n",
    "Evaluate different approaches for accuracy, speed, and cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_extractors(articles: List[Dict]) -> Dict:\n",
    "    \"\"\"Benchmark different extraction methods.\"\"\"\n",
    "    \n",
    "    results = {\n",
    "        'methods_tested': [],\n",
    "        'performance': {},\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    extraction_chain = FreeLLMExtractionChain()\n",
    "    \n",
    "    for i, article in enumerate(articles[:2]):  # Test on first 2 articles\n",
    "        text = f\"{article.get('title', '')} {article.get('description', '')}\"\n",
    "        \n",
    "        print(f\"\\n--- Testing Article {i+1} ---\")\n",
    "        print(f\"Text: {text[:100]}...\")\n",
    "        \n",
    "        # Time the extraction\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        entities = extraction_chain.extract_entities_with_fallback(text)\n",
    "        relationships = extraction_chain.extract_relationships_with_fallback(text, entities)\n",
    "        \n",
    "        extraction_time = time.time() - start_time\n",
    "        \n",
    "        results['performance'][f'article_{i+1}'] = {\n",
    "            'entities_count': len(entities),\n",
    "            'relationships_count': len(relationships),\n",
    "            'extraction_time': round(extraction_time, 2),\n",
    "            'entities': entities,\n",
    "            'relationships': relationships\n",
    "        }\n",
    "        \n",
    "        print(f\"Extraction completed in {extraction_time:.2f}s\")\n",
    "        print(f\"Found {len(entities)} entities, {len(relationships)} relationships\")\n",
    "    \n",
    "    # Add method availability\n",
    "    results['methods_tested'] = extraction_chain.available_extractors\n",
    "    \n",
    "    # Generate recommendations\n",
    "    recommendations = []\n",
    "    \n",
    "    if 'ollama' in extraction_chain.available_extractors:\n",
    "        recommendations.append(\"‚úì Ollama: Best for privacy, local inference, no API costs\")\n",
    "    else:\n",
    "        recommendations.append(\"‚ö†Ô∏è Consider installing Ollama for local LLM inference\")\n",
    "    \n",
    "    if 'huggingface' in extraction_chain.available_extractors:\n",
    "        recommendations.append(\"‚úì HuggingFace: Good accuracy for NER tasks, local processing\")\n",
    "    else:\n",
    "        recommendations.append(\"‚ö†Ô∏è Consider installing transformers for better NER accuracy\")\n",
    "    \n",
    "    if len(extraction_chain.available_extractors) == 0:\n",
    "        recommendations.append(\"‚ö†Ô∏è No advanced extractors available - using rule-based fallback only\")\n",
    "    \n",
    "    recommendations.extend([\n",
    "        \"üí° For production: Use Ollama + HuggingFace as primary, rule-based as fallback\",\n",
    "        \"üí° For development: API services offer good quality without local setup\",\n",
    "        \"üí° For high volume: Local models (Ollama) avoid API rate limits and costs\"\n",
    "    ])\n",
    "    \n",
    "    results['recommendations'] = recommendations\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run benchmark\n",
    "if sample_articles:\n",
    "    benchmark_results = benchmark_extractors(sample_articles)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä BENCHMARK RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nMethods Available: {benchmark_results['methods_tested']}\")\n",
    "    \n",
    "    print(\"\\nPerformance Summary:\")\n",
    "    for article_key, stats in benchmark_results['performance'].items():\n",
    "        print(f\"  {article_key}: {stats['entities_count']} entities, {stats['relationships_count']} relationships ({stats['extraction_time']}s)\")\n",
    "    \n",
    "    print(\"\\nRecommendations:\")\n",
    "    for rec in benchmark_results['recommendations']:\n",
    "        print(f\"  {rec}\")\n",
    "else:\n",
    "    print(\"No sample articles available for benchmarking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Integration with Existing Pipeline\n",
    "\n",
    "Show how to integrate free LLM extraction into the existing Scrantenna pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_production_extractor():\n",
    "    \"\"\"Create a production-ready extractor for integration.\"\"\"\n",
    "    \n",
    "    extractor_code = '''\n",
    "# Add this to shorts/generate_shorts.py or create as separate module\n",
    "\n",
    "class ProductionFreeLLMExtractor:\n",
    "    \"\"\"Production-ready free LLM entity/relationship extractor.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.ollama_available = self._check_ollama()\n",
    "        self.hf_available = self._check_huggingface()\n",
    "        \n",
    "    def _check_ollama(self) -> bool:\n",
    "        try:\n",
    "            import ollama\n",
    "            ollama.show(\"llama3.2:3b\")  # Check if model exists\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def _check_huggingface(self) -> bool:\n",
    "        try:\n",
    "            from transformers import pipeline\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def extract_for_article(self, article: Dict) -> Dict:\n",
    "        \"\"\"Extract entities and relationships for a news article.\"\"\"\n",
    "        text = f\"{article.get('title', '')} {article.get('description', '')}\"\n",
    "        \n",
    "        # Extract entities\n",
    "        entities = self._extract_entities_with_fallback(text)\n",
    "        \n",
    "        # Extract relationships  \n",
    "        relationships = self._extract_relationships_with_fallback(text, entities)\n",
    "        \n",
    "        # Generate SVG visualization\n",
    "        svg_graph = self._generate_svg_graph(entities, relationships)\n",
    "        \n",
    "        return {\n",
    "            \"entities\": entities,\n",
    "            \"relationships\": relationships, \n",
    "            \"svg\": svg_graph,\n",
    "            \"method\": self._get_active_method()\n",
    "        }\n",
    "    \n",
    "    def _extract_entities_with_fallback(self, text: str) -> List[Dict]:\n",
    "        # Try Ollama first\n",
    "        if self.ollama_available:\n",
    "            try:\n",
    "                return self._ollama_extract_entities(text)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Try HuggingFace\n",
    "        if self.hf_available:\n",
    "            try:\n",
    "                return self._hf_extract_entities(text)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Fallback to rules\n",
    "        return self._rule_based_entities(text)\n",
    "    \n",
    "    def _get_active_method(self) -> str:\n",
    "        if self.ollama_available:\n",
    "            return \"ollama_llama3.2\"\n",
    "        elif self.hf_available:\n",
    "            return \"huggingface_bert\"\n",
    "        else:\n",
    "            return \"rule_based\"\n",
    "            \n",
    "# Usage in generate_shorts.py:\n",
    "# extractor = ProductionFreeLLMExtractor()\n",
    "# graph_data = extractor.extract_for_article(article)\n",
    "'''\n",
    "    \n",
    "    print(\"üîß PRODUCTION INTEGRATION CODE:\")\n",
    "    print(extractor_code)\n",
    "    \n",
    "    # Save to file for easy integration\n",
    "    with open('../shorts/free_llm_extractor.py', 'w') as f:\n",
    "        f.write(extractor_code.strip())\n",
    "    \n",
    "    print(\"\\n‚úÖ Saved production extractor to: ../shorts/free_llm_extractor.py\")\n",
    "    print(\"\\nTo integrate:\")\n",
    "    print(\"1. Copy the code to your project\")\n",
    "    print(\"2. Install dependencies: pip install ollama transformers torch\")\n",
    "    print(\"3. Pull Ollama model: ollama pull llama3.2:3b\")\n",
    "    print(\"4. Replace existing extraction in generate_shorts.py\")\n",
    "\n",
    "create_production_extractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Recommendations\n",
    "\n",
    "### Free LLM Options for Entity/Relationship Extraction:\n",
    "\n",
    "1. **Ollama (Recommended)**\n",
    "   - ‚úÖ Completely free and local\n",
    "   - ‚úÖ No API keys or rate limits\n",
    "   - ‚úÖ Privacy-preserving (no data sent externally)\n",
    "   - ‚úÖ Good accuracy with Llama 3.2\n",
    "   - ‚ö†Ô∏è Requires local GPU/CPU resources\n",
    "\n",
    "2. **HuggingFace Transformers**\n",
    "   - ‚úÖ Excellent NER accuracy\n",
    "   - ‚úÖ Local processing\n",
    "   - ‚úÖ Well-established models\n",
    "   - ‚ö†Ô∏è Limited to NER (entities only)\n",
    "\n",
    "3. **Free API Services**\n",
    "   - ‚úÖ No local setup required\n",
    "   - ‚úÖ Good for development/testing\n",
    "   - ‚ö†Ô∏è Rate limits and quotas\n",
    "   - ‚ö†Ô∏è Data privacy concerns\n",
    "\n",
    "### Cost Comparison:\n",
    "- **OpenAI GPT**: ~$0.01-0.03 per 1000 tokens\n",
    "- **Ollama**: $0 (one-time hardware cost)\n",
    "- **HuggingFace Transformers**: $0 (local processing)\n",
    "- **Free APIs**: $0 with usage limits\n",
    "\n",
    "### Recommended Implementation:\n",
    "1. **Primary**: Ollama with Llama 3.2 (3B or 7B)\n",
    "2. **Secondary**: HuggingFace BERT for NER\n",
    "3. **Fallback**: Rule-based extraction\n",
    "\n",
    "This provides a robust, cost-effective pipeline that maintains quality while eliminating API dependency and costs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}